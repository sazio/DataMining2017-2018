{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Mining:<br>Statistical Modeling and Learning from Data\n",
    "\n",
    "## Dr. Ciro Cattuto<br>Dr. Laetitia Gauvin<br>Dr. André Panisson\n",
    "\n",
    "### Exercises - Topic modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract topics from unstructured texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from sklearn import datasets, decomposition\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 1000\n",
    "n_topics = 6\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset\n",
    "The 20 Newsgroups data set\n",
    "\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups\n",
    "The data is organized into 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware / comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale / soc.religion.christian). Here is a list of the 20 newsgroups, partitioned (more or less) according to subject matter: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.fetch_20newsgroups(shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: timmbake@mcl.ucsb.edu (Bake Timmons)\n",
      "Subject: Re: Amusing atheists and agnostics\n",
      "Lines: 66\n",
      "\n",
      "\n",
      "James Hogan writes:\n",
      "\n",
      "timmbake@mcl.ucsb.edu (Bake Timmons) writes:\n",
      ">>Jim Hogan quips:\n",
      "\n",
      ">>... (summary of Jim's stuff)\n",
      "\n",
      ">>Jim, I'm afraid _you've_ missed the point.\n",
      "\n",
      ">>>Thus, I think you'll have to admit that  atheists have a lot\n",
      ">>more up their sleeve than you might have suspected.\n",
      "\n",
      ">>Nah.  I will encourage people to learn about atheism to see how little atheists\n",
      ">>have up their sleeves.  Whatever I might have suspected is actually quite\n",
      ">>meager.  If you want I'll send them your address to learn less about your\n",
      ">>faith.\n",
      "\n",
      ">Faith?\n",
      "\n",
      "Yeah, do you expect people to read the FAQ, etc. and actually accept hard\n",
      "atheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\n",
      "of steam!\n",
      "\n",
      ">>>Fine, but why do these people shoot themselves in the foot and mock\n",
      ">>>the idea of a God?  ....\n",
      "\n",
      ">>>I hope you understand now.\n",
      "\n",
      ">>Yes, Jim.  I do understand now.  Thank you for providing some healthy sarcasm\n",
      ">>that would have dispelled any sympathies I would have had for your faith.\n",
      "\n",
      ">Bake,\n",
      "\n",
      ">Real glad you detected the sarcasm angle, but am really bummin' that\n",
      ">I won't be getting any of your sympathy.  Still, if your inclined\n",
      ">to have sympathy for somebody's *faith*, you might try one of the\n",
      ">religion newsgroups.\n",
      "\n",
      ">Just be careful over there, though. (make believe I'm\n",
      ">whispering in your ear here)  They're all delusional!\n",
      "\n",
      "Jim,\n",
      "\n",
      "Sorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\n",
      "denial about the faith you need to get by.  Oh well, just pretend that it will\n",
      "all end happily ever after anyway.  Maybe if you start a new newsgroup,\n",
      "alt.atheist.hard, you won't be bummin' so much?\n",
      "\n",
      ">Good job, Jim.\n",
      ">.\n",
      "\n",
      ">Bye, Bake.\n",
      "\n",
      "\n",
      ">>[more slim-Jim (tm) deleted]\n",
      "\n",
      ">Bye, Bake!\n",
      ">Bye, Bye!\n",
      "\n",
      "Bye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \n",
      "--\n",
      "Bake Timmons, III\n",
      "\n",
      "-- \"...there's nothing higher, stronger, more wholesome and more useful in life\n",
      "than some good memory...\" -- Alyosha in Brothers Karamazov (Dostoevsky)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (dataset.data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From News to Feature Vectors\n",
    "We need to transform our text data into feature vectors, numerical representations which are suitable for performing statistical analysis. The most common way to do this is to apply a bag-of-words approach where the frequency of an occurrence of a word becomes a feature for our classifier.\n",
    "\n",
    "\n",
    "## Term Frequency-Inverse Document Frequency\n",
    "\n",
    "We want to consider the relative importance of particular words, so we'll use term frequency–inverse document frequency as a weighting factor. This will control for the fact that some words are more \"spamy\" than others.\n",
    "\n",
    "## Mathematical details\n",
    "https://en.wikipedia.org/wiki/Tf–idf\n",
    "\n",
    "tf–idf is the product of two statistics, term frequency and inverse document\n",
    "frequency. Various ways for determining the exact values of both statistics\n",
    "exist. In the case of the '''term frequency''' tf(''t'',''d''), the simplest\n",
    "choice is to use the ''raw frequency'' of a term in a document, i.e. the\n",
    "number of times that term ''t'' occurs in document ''d''. If we denote the raw\n",
    "frequency of ''t'' by f(''t'',''d''), then the simple tf scheme is\n",
    "tf(''t'',''d'') = f(''t'',''d''). Other possibilities\n",
    "include:\n",
    "\n",
    "  * boolean_data_type \"frequencies\": tf(''t'',''d'') = 1 if ''t'' occurs in ''d'' and 0 otherwise; \n",
    "  * logarithmically scaled frequency: tf(''t'',''d'') = log (f(''t'',''d'') + 1); \n",
    "  * augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the maximum raw frequency of any term in the document: :$\\mathrm{tf}(t,d) = 0.5 + \\frac{0.5 \\times \\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(w, d):w \\in d\\}}$\n",
    "\n",
    "The '''inverse document frequency''' is a measure of whether the term is\n",
    "common or rare across all documents. It is obtained by dividing the total\n",
    "number of documents by the number of documents containing the\n",
    "term, and then taking the logarithm of that quotient.\n",
    "\n",
    "$$\\mathrm{idf}(t, D) = \\log \\frac{|D|}{|\\{d \\in D: t \\in d\\}|}$$\n",
    "\n",
    "with\n",
    "\n",
    "  * $|D| $: cardinality of D, or the total number of documents in the corpus \n",
    "  * $|\\{d \\in D: t \\in d\\}|$ : number of documents where the term $t$ appears (i.e., $\\mathrm{tf}(t,d) eq 0$). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the formula to $1 + |\\{d \\in D: t \\in d\\}|$. \n",
    "\n",
    "Mathematically the base of the log function does not matter and constitutes a\n",
    "constant multiplicative factor towards the overall result.\n",
    "\n",
    "Then tf–idf is calculated as\n",
    "\n",
    "$$\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\times \\mathrm{idf}(t, D)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the data using the most common words\n",
    "# normalize with TF-IDF weighting (without top 5% stop words) --> max_df  =0.95\n",
    "\n",
    "vectorizer = text.CountVectorizer(max_df=0.95,\n",
    "                                  max_features=n_features,\n",
    "                                  stop_words='english')  # --> scarta una lista di parole già confezionata (per la lingua inglese)\n",
    "counts = vectorizer.fit_transform(dataset.data[:n_samples])  #vettorizzo i primi 1000 testi del dataset\n",
    "tfidf = text.TfidfTransformer().fit_transform(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model on with n_samples=1000 and n_features=1000...\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "# use of NMF in text mining \n",
    "\n",
    "print(\"Fitting the NMF model on with n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "nmf = decomposition.NMF(n_components=n_topics)    # n_topics = 6\n",
    "\n",
    "nmf.fit(tfidf)\n",
    "W = nmf.transform(tfidf)   # ---> matrice L: 1000x6\n",
    "H = nmf.components_        # ---> matrice R: 6x1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names è un vett. che contiene le parole in ordine crescente di punteggio. ---> se voglio le parole \n",
    "# che caratterizzano l'argomento devo prendere le ultime\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the top n words in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "people god don think like just good time way know say life israel jesus christian bible want did does going\n",
      "\n",
      "Topic #1:\n",
      "edu university cs host posting article nntp writes cc reply distribution cwru state uiuc game john washington new baseball michael\n",
      "\n",
      "Topic #2:\n",
      "com hp article writes netcom sun corp stratus posting ca nntp host portal news jim att org distribution systems support\n",
      "\n",
      "Topic #3:\n",
      "windows uk ac help drive problem thanks use monitor dos software using card file window mail color application pc drivers\n",
      "\n",
      "Topic #4:\n",
      "clipper key chip encryption government keys public secure use enforcement house law secret brad standard algorithm phone people pat security\n",
      "\n",
      "Topic #5:\n",
      "nasa gov space jpl center research shuttle moon program laboratory earth distribution henry brian data article sci world long posting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, topic in enumerate(H):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join( [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] ) )    # n_top_words = 20\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
