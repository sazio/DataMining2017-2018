{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Mining:<br>Statistical Modeling and Learning from Data\n",
    "\n",
    "## Dr. Ciro Cattuto<br>Dr. Laetitia Gauvin<br>Dr. André Panisson\n",
    "\n",
    "### Exercises - Text Message Spam Filter\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Text Message Spam Filter with a Naive Bayes Classifier\n",
    "\n",
    "This tutorial explains how to classify text messages as spam / not spam using scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset\n",
    "The SMS Spam Collection is open source and available at the UCI Machine Learning Repository.\n",
    "The data files and documentation can be found here: http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "\n",
    "## Reading the Dataset into Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4827 Not Spam\n",
      "+ 747 Spam\n",
      " ---------\n",
      " 5574 Total\n",
      "Proportion spam: 13.40/100\n"
     ]
    }
   ],
   "source": [
    "#lavoriamo sul riconoscimento di testo\n",
    "#dobbiamo decidere se le mail che arrivano sono spam o no\n",
    "\n",
    "messages = []\n",
    "categories = []\n",
    "i = 0\n",
    "for line in open(\"data/smsdata.txt\"):      #smsdata.txt contiene una serie di messaggi, preceduti dall'etichetta della\n",
    "                                           #classificazione ('spam' oppure 'ham')\n",
    "    category, message = line.split('\\t')\n",
    "    messages.append(message)\n",
    "    categories.append(category)\n",
    "\n",
    "y = np.array([0 if item==\"ham\" else 1 for item in categories])\n",
    " \n",
    "print\n",
    "print (\" %d Not Spam\" % (y==0).sum())\n",
    "print (\"+ %d Spam\" % (y==1).sum())\n",
    "print (\" ---------\")\n",
    "print (\" %d Total\" % len(y))\n",
    "print \n",
    "print (\"Proportion spam: %.2f/100\" % (100.*(y==1).sum() / float(len(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open(\"data/smsdata.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Text Messages to Feature Vectors\n",
    "We need to transform our text data into feature vectors, numerical representations which are suitable for performing statistical analysis. The most common way to do this is to apply a bag-of-words approach where the frequency of an occurrence of a word becomes a feature for our classifier.\n",
    "\n",
    "COMMENTI:\n",
    "n = numero di testi (mail)\n",
    "d = dizionario, numero di parole che utilizziamo per valutare i testi\n",
    "\n",
    "un dizionario di 10000 parole è piccolo. però 10000 parole per 5000 mail è una cosa troppo grossa da trattare. Ma non ci interessa dare tutti i valori di questa matrice. utilizziamo il metodo delle matrici sparse,ci sono già nelle librerie.\n",
    "Parole molto comuni non servono per la classificazione.\n",
    "Controlliamo frequenza dei termini nei documenti, e la moltiplichiamo per un fattore --> il peso deve essere piccolo se la parola è poco informativa, più grande se la parola è indicativa. Quindi usiamo come peso l'inverso della frequenza.\n",
    "I dettagli sono spiegati, leggili.\n",
    "\n",
    "## Term Frequency-Inverse Document Frequency\n",
    "\n",
    "We want to consider the relative importance of particular words, so we'll use term frequency–inverse document frequency as a weighting factor. This will control for the fact that some words are more \"spamy\" than others.\n",
    "\n",
    "## Mathematical details\n",
    "\n",
    "tf–idf is the product of two statistics, term frequency and inverse document\n",
    "frequency. Various ways for determining the exact values of both statistics\n",
    "exist. In the case of the '''term frequency''' tf(''t'',''d''), the simplest\n",
    "choice is to use the ''raw frequency'' of a term in a document, i.e. the\n",
    "number of times that term ''t'' occurs in document ''d''. If we denote the raw\n",
    "frequency of ''t'' by f(''t'',''d''), then the simple tf scheme is\n",
    "tf(''t'',''d'') = f(''t'',''d''). Other possibilities\n",
    "include:\n",
    "\n",
    "  * boolean_data_type \"frequencies\": tf(''t'',''d'') = 1 if ''t'' occurs in ''d'' and 0 otherwise; \n",
    "  * logarithmically scaled frequency: tf(''t'',''d'') = log (f(''t'',''d'') + 1); \n",
    "  * augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the maximum raw frequency of any term in the document: :$\\mathrm{tf}(t,d) = 0.5 + \\frac{0.5 \\times \\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(w, d):w \\in d\\}}$\n",
    "\n",
    "The '''inverse document frequency''' is a measure of whether the term is\n",
    "common or rare across all documents. It is obtained by dividing the total\n",
    "number of documents by the number of documents containing the\n",
    "term, and then taking the logarithm of that quotient.\n",
    "\n",
    ":$\\mathrm{idf}(t, D) = \\log \\frac{|D|}{|\\{d \\in D: t \\in d\\}|}$\n",
    "\n",
    "LOG SERVE PER SPAZIARE MEGLIO QUESTI VALORI, CHE ALTRIMENTI SAREBBERO TUTTI ALTI. POTREMMO ANCHE RIMUOVERE PAROLI TROPPO COMUNI.\n",
    "\n",
    "with\n",
    "\n",
    "  * $|D|$: cardinality of D, or the total number of documents in the corpus \n",
    "  * $|\\{d \\in D: t \\in d\\}|$ : number of documents where the term $ t $ appears (i.e., $\\mathrm{tf}(t,d) eq 0$). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the formula to $1 + |\\{d \\in D: t \\in d\\}|$. \n",
    "\n",
    "Mathematically the base of the log function does not matter and constitutes a\n",
    "constant multiplicative factor towards the overall result.\n",
    "\n",
    "Then tf–idf is calculated as\n",
    "\n",
    "$$\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\times \\mathrm{idf}(t, D)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train is a sparse matrix with shape: (5574, 83964)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "#UTILIZZEREMO queste due classi: la prima prende un testo e lo trasforma \n",
    "#in un vettore di conteggi. la seconda applica la trasformazione tfidf al vettore, cioè lo proietta sul sottospazio\n",
    "#degli argomenti più comuni\n",
    "\n",
    "# regular expression \n",
    "pattern ='(?u)\\\\b[A-Za-z]{3,}' \n",
    "#cerca solo parole con tre o più caratteri: {3,}\n",
    "#tutte le parole nel formato a-z o A-Z: [A-Z a-z]\n",
    "#ogni limite di parola: \\b\n",
    "#tutte le parole con questo formato saranno incluse nel dizionario\n",
    "\n",
    "#per considerare anche 'ordine delle parole, nel dizionario non metto solo \n",
    "#le parole ma anche gli n-grammi, cioè serie di due parole in ordine e \n",
    "#serie di tre parole in ordine ( in questo caso bi-grammi e tri-grammi)\n",
    "\n",
    "cv = CountVectorizer(stop_words=None, token_pattern=pattern,     #l'espressione regolare da riconoscere è un pattern\n",
    "                     ngram_range=(1, 3))                         #limite min e max dell'engramma\n",
    "C = cv.fit_transform(messages)\n",
    "\n",
    "tfidf = TfidfTransformer(sublinear_tf=True)   #riscalo i pesi con una funz. sublineare, es: 1+log(p)\n",
    "#tfidf = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\n",
    "                        \n",
    "#calculate features using tf-idf and create a training set \n",
    "X_train = tfidf.fit_transform(C)\n",
    "print \n",
    "print (\"X_train is a sparse matrix with shape: %s\" % str(X_train.shape))\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5574x83964 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 164008 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a variable, tfidf, which is a vectorizer responsible for performing three important steps:\n",
    "\n",
    "- First, it will build a dictionary of features where keys are terms and values are indices of the term in the feature matrix (that's the fit part in fit_transform)\n",
    "- Second, it will transform our documents into numerical feature vectors according to the frequency of words appearing in each text message. Since any one text message is short, each feature vector will be made up of mostly zeros, each of which indicates that a given word appeared zero times in that message.\n",
    "- Lastly, it will compute the tf-idf weights for our term frequency matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Using Bayes' theorem, the conditional probability of observing a class $C_k$ given that we observed a set of features $\\mathbf{x}$ can be decomposed as\n",
    "\n",
    "$$p(C_k \\mid \\mathbf{x}) = \\frac{p(C_k) \\ p(\\mathbf{x} \\mid C_k)}{p(\\mathbf{x})}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bank example\n",
    "\n",
    "PC(Previous Client), CR(Criminal Record), Age, MP(Missed Payments), Res(Result of classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n",
    "   <colgroup>\n",
    "        <col width=\"103\">\n",
    "        <col width=\"103\">\n",
    "        <col width=\"103\">\n",
    "        <col width=\"103\">\n",
    "        <col width=\"103\">\n",
    "        <col width=\"103\">\n",
    "    </colgroup>\n",
    "\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>Id</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>PC</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>CR</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>Age</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>MP</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>Res</p>\n",
    "            </td>\n",
    "        </tr>\n",
    "\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>1</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>1</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>50</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>E</p>\n",
    "            </td>\n",
    "        </tr>\n",
    "\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>2</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>1</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>28</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>E</p>\n",
    "            </td>\n",
    "        </tr>\n",
    "\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>3</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>35</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>E</p>\n",
    "            </td>\n",
    "        </tr>\n",
    "\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>4</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>55</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>E</p>\n",
    "            </td>\n",
    "        </tr>\n",
    "\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>5</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>49</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>E</p>\n",
    "            </td>\n",
    "        </tr>\n",
    "\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>6</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>1</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>75</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>NE</p>\n",
    "            </td>\n",
    "        </tr>\n",
    "\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>7</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>1</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>38</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>10</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>NE</p>\n",
    "            </td>\n",
    "        </tr>\n",
    "\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>8</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>83</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>NE</p>\n",
    "            </td>\n",
    "        </tr>\n",
    "\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>9</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>1</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>1</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>44</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>5</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>NE</p>\n",
    "            </td>\n",
    "        </tr>\n",
    "\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>10</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>1</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>28</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>0</p>\n",
    "            </td>\n",
    "\n",
    "            <td>\n",
    "                <p>NE</p>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given that \n",
    "\n",
    "$$p(\\mathbf{x} \\mid C_k)  =  \\prod_{i=1}^{D} p(x_i \\mid C_k) $$\n",
    "\n",
    "then we have that\n",
    "\n",
    "$$p(C_k \\mid \\mathbf{x}) \\propto p(C_k) \\ \\prod_{i=1}^{D} p(x_i \\mid C_k) $$\n",
    "\n",
    "and then we choose the predicted class by:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_{C_k} p({C_k}) \\prod_{i=1}^{D} p(x_i \\mid {C_k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to predict the class for the following cases:\n",
    "\n",
    "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" class=\"Table2\">\n",
    "    <colgroup>\n",
    "        <col width=\"103\">\n",
    "        <col width=\"103\">\n",
    "        <col width=\"103\">\n",
    "        <col width=\"103\">\n",
    "        <col width=\"103\">\n",
    "        <col width=\"103\">\n",
    "    </colgroup>\n",
    "\n",
    "    <tr class=\"Table21\">\n",
    "        <td>\n",
    "            <p class=\"P4\">Id</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P4\">PC</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P4\">CR</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P4\">Age</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P4\">MP</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P4\">Res</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "    <tr class=\"Table21\">\n",
    "        <td>\n",
    "            <p class=\"P3\">11</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P3\">0</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P3\">0</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P3\">50</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P3\">0</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P4\">?</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "    <tr class=\"Table21\">\n",
    "        <td>\n",
    "            <p class=\"P3\">12</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P3\">1</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P3\">0</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P3\">80</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P3\">0</p>\n",
    "        </td>\n",
    "\n",
    "        <td>\n",
    "            <p class=\"P4\">?</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From:\n",
    "\n",
    "- $\\mathbf{C} \\in \\{\\mathbf{E}, \\mathbf{NE}\\}$, $p(\\mathbf{E}) = 1/2$, $p(\\mathbf{NE}) = 1/2$\n",
    "\n",
    "\n",
    "- $\\mathbf{PC} \\in \\{0, 1\\}$, $p(0 \\mid \\mathbf{E}) = 3/5$, $p(1 \\mid \\mathbf{E}) = 2/5$, $p(0 \\mid \\mathbf{NE}) = 2/5$, $p(1 \\mid \\mathbf{NE}) = 3/5$\n",
    "\n",
    "\n",
    "- $\\mathbf{CR} \\in \\{0, 1\\}$, $p(0 \\mid \\mathbf{E}) = 1$, $p(1 \\mid \\mathbf{E}) = 0$, $p(0 \\mid \\mathbf{NE}) = 3/5$, $p(1 \\mid \\mathbf{NE}) = 2/5$\n",
    "\n",
    "\n",
    "- $\\mathbf{Age} \\in \\{\\lt 65, \\gt 65 \\}$, $p(\\lt 65 \\mid \\mathbf{E}) = 1$, $p(\\gt 65 \\mid \\mathbf{E}) = 0$, $p(\\lt 65 \\mid \\mathbf{NE}) = 3/5$, $p(\\gt 65 \\mid \\mathbf{NE}) = 2/5$\n",
    "\n",
    "\n",
    "- $\\mathbf{MP} \\in \\{0, \\gt 0 \\}$, $p(0 \\mid \\mathbf{E}) = 1$, $p(\\gt 0 \\mid \\mathbf{E}) = 0$, $p(0 \\mid \\mathbf{NE}) = 3/5$, $p(\\gt 0 \\mid \\mathbf{NE}) = 2/5$\n",
    "\n",
    "For object 11, we have that:\n",
    "\n",
    "$p(\\mathbf{E} \\mid [ 0, 0, \\lt 65, 0] =  1/2 \\times 3/5 \\times 1 \\times 1 \\times 1 = 0.3$\n",
    "\n",
    "$p(\\mathbf{NE} \\mid [ 0, 0, \\lt 65, 0] = 1/2 \\times 2/5 \\times 3/5 \\times 3/5 \\times 3/5 = 0.04$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes can be modeled in several different ways. For example, to model a feature using a **normal** density function, we have that:\n",
    "\n",
    "$$p(x_i \\mid C_k)  =  \\frac{1}{{\\sigma_{ik} \\sqrt {2\\pi } }} e^{{ - \\left( {x - \\mu_{ik} } \\right)^2 } / {2\\sigma_{ik} ^2 }} $$\n",
    "\n",
    "and to model a feature using a **multinomial** density function, we have that the multinomial naive Bayes classifier becomes a linear classifier when expressed in log-space:\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(C_k|\\mathbf{x}) & \\varpropto \\log \\left( p(C_k) \\prod_{i=1}^n {p_{ki}}^{x_i} \\right) \\\\\n",
    "                       & = \\log p(C_k) + \\sum_{i=1}^n x_i \\cdot \\log p_{ki}                 \\\\\n",
    "                       & = b + \\mathbf{w}_k^\\top \\mathbf{x}\n",
    "\\end{align}\n",
    "\n",
    "where $b = \\log p(C_k)$ and $w_{ki} = \\log p_{ki}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using SciKits' MultinomialNB, a Naive Bayes classifier effective for catching spam with the added benefits of scalability and low training time.\n",
    "\n",
    "MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors $\\mathbf{w}_y = ( w_{y1},\\ldots, w_{yD})$ for each class $y$, where $D$ is the number of features (in text classification, the size of the vocabulary) and $w_{yi}$ is the probability $P(x_i \\mid y)$ of feature $i$ appearing in a sample belonging to class $y$.\n",
    "The parameters $\\mathbf{w}_y$ are estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:\n",
    "$w_{yi} = \\frac{ N_{yi} + \\alpha}{N_y + \\alpha n}$\n",
    "where $N_{yi} = \\sum_{x \\in T} x_i$ is the number of times feature $i$ appears in a sample of class $y$ in the training set $T$, and $N_{y} = \\sum_{i=1}^{|T|} N_{yi}$ is the total count of all features for class $y$.\n",
    "The smoothing priors $\\alpha \\ge 0$ accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting $\\alpha = 1$ is called *Laplace smoothing*, while $\\alpha < 1$ is called *Lidstone smoothing*.\n",
    "\n",
    "**Why Multinomial Naive Bayes**?   \n",
    "Each row of the training set represents a document. A document is a list of $n$ words. If we consider that each word $w_i$ of the vocabulary appears in the vocabulary with probability $p_i$, then each document can be represented as a **multinomial distribution** with $n$ trials. The number of possible outcomes is the number of words in the vocabulary, and in each trial we choose a word from the vocabulary following the event probabilities $p_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train is a list of categories: [0 0 1 ... 0 0 0] ...\n",
      "X_train has 5574 feature vectors\n",
      "y_train has 5574 target classes\n",
      "dataset has 5574 rows\n",
      "Trained MultinomialNB Classifier\n",
      "Coefficients: [[-11.39500839 -11.39500839 -11.39500839 ... -11.39500839 -11.39500839 ...\n",
      "   Intercept: [-2.00980302]\n"
     ]
    }
   ],
   "source": [
    "#assunzione: la distribuzione delle parole segue una distribuzione \n",
    "#multinomiale --> Il mio testo è fatto di n parole su d possibili tra cui\n",
    "#scegliere\n",
    "\n",
    "#sopra ci sono dettagi su come vengono calcolti i parametri. Interessante:\n",
    "#la prob del dato, data la classe y, la otteniamo tramite pesi calcolati\n",
    "# con la formula W(yi)= (Nyi + alfa) / (Ny + alfa*n)\n",
    "\n",
    "\n",
    "#create a list of training labels. 1 is spam, 0 if ham\n",
    "y_train = y\n",
    " \n",
    "print (\"y_train is a list of categories: %s ...\" % str(y_train)[:70])\n",
    "print (\"X_train has %d feature vectors\" % (X_train.shape[0]))\n",
    "print (\"y_train has %d target classes\" %(len(y_train)))\n",
    "print (\"dataset has %d rows\" %(len(messages)))\n",
    "print\n",
    " \n",
    "# create a Naive Bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    " \n",
    "#qua manca la parte di model_selection\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print (\"Trained MultinomialNB Classifier\")\n",
    "print (\"Coefficients: %s ...\" % (str(clf.coef_)[:70]))\n",
    "print (\"   Intercept: %s\" %(str(clf.intercept_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a variable, y_train, which is simply a list of target classes which our classifier will be trained to identify. 1 indicates spam while 0 indicates ham, or non-spam.\n",
    "\n",
    "Then we fit the model by passing the X_train sparse matrix and y_train to our MultinomialNB classifier's fit function.\n",
    "\n",
    "#Classifying New Observations\n",
    "\n",
    "Now let's classify the test documents as spam or not spam and see how we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "test_messages = [\"Call MobilesDirect free on 08000938767 to update now! or2stoptxt\",\n",
    "                 \"Call now for a free trial offer!\",\n",
    "                 \"Hey Sam, want to get some pizza later?\",\n",
    "                 \"idk my bff jill?\",\n",
    "                 \"Free later for a beer? Call me now!\"]\n",
    "#primi 2 messaggi sono di spam, gli altri 3 no\n",
    "                 \n",
    "# extract features from raw text documents\n",
    "C_test = cv.transform(test_messages)  # vettorizzo i messaggi\n",
    "X_test = tfidf.transform(C_test)      # proietto i vettori con il tfidf\n",
    " \n",
    "# MultinomialNB's predict classes directly\n",
    "print (\"Classified: %s\" % clf.predict(X_test))  #classifico i vettori con il MultinomialNB che ho allenato\n",
    "\n",
    "# risultato: ci dice che nessuno dei sondaggi è spam.\n",
    "# dobbamo fare un passo di model selection per trovare i parametri giusti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict function yields an array of True / False values (True for spam, False for not spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "The classification results for the test documents are not very encouraging.   \n",
    "Find the best parameter for the MultinomialNB model, and check the classification results for the test documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in qesto esercizio dobbiamo fare la cosa di prima aggiungendo il \n",
    "#model_selection    \n",
    "\n",
    "# MODO 1\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alphas = np.logspace(-4, 0, 50)\n",
    "model = MultinomialNB()\n",
    "gs = GridSearchCV(model, param_grid={'alpha': alphas}, cv=10)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vettore dei punteggi:  [0.93, 0.928, 0.925, 0.92, 0.917, 0.915, 0.912, 0.909, 0.905, 0.901, 0.897, 0.891, 0.884, 0.878, 0.871, 0.863, 0.855, 0.846, 0.839, 0.831, 0.822, 0.812, 0.804, 0.797, 0.793, 0.79, 0.787, 0.788, 0.79, 0.797, 0.806, 0.819, 0.834, 0.857, 0.878, 0.901, 0.92, 0.942, 0.959, 0.972, 0.98, 0.983, 0.984, 0.984, 0.981, 0.975, 0.97, 0.962, 0.952, 0.943]\n",
      "best score ha indice:  42\n",
      "\n",
      "best alpha:  0.2682695795279725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODO 2\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "alphas = np.logspace(-4, 0, 50)\n",
    "model = MultinomialNB()\n",
    "scores = []\n",
    "for alpha in alphas:\n",
    "    model.alpha = alpha\n",
    "    cv_scores = cross_val_score(model, X_train,\n",
    "                                y_train, cv=10)\n",
    "    scores.append(np.mean(cv_scores))\n",
    "    \n",
    "best_alpha = alphas[np.argmax(scores)]\n",
    "print (\"vettore dei punteggi: \", [i.round(3) for i in scores])\n",
    "print (\"best score ha indice: \", np.argmax(scores))\n",
    "print ()\n",
    "print (\"best alpha: \", best_alpha)\n",
    "\n",
    "model = MultinomialNB(alpha=best_alpha)\n",
    "model.fit(X_train, y_train)\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
